Title: About 

## About The Blog

The blog's name is a a twist on "The Elements of Statistical Learning", the famous book by Hastie, Tibshirani, Friedman that gave me my first foundation in ML. With the advent of machine learning, GPU's and neural nets of all kinds, there has been a loss in skepticism that statisticians always include in their results. As this field has exploded and grown beyond just the computer scientists and statisticans, the confidence interval has disappeared (and rightfully so), but the pesimissim that pervaded any kind of prediction and inference has gone with it, which I consider a loss. All models are imperfect (but some are useful) and we need to convey that pessimism rather pulling a George W "We Did It" after a successful set of results.

This blog aims at looking at bringing skepticism back to statistics and ML and looks at not just results, but hidden costs to using the fanciest model on the market. Maintenance of models, deployment and interpretability for someone not techincally versed are big issues that nobody ever discusses in this industry and are things that should be considered when approaching problems. Recently, the slew of data makes it very easy to generate biased models that reflect the bias of the underlying hummans/data and this is an area Data Scientist and ML Engineers are not taking seriously yet.

This is github after all, so this will be mostly techincally focussed. 


## About Me

I'm Varun, I was a Data Scientist before the term existed. My first job title was DSP research, I then called it Data Mining for a while, graduated to R&D or analyst before the term Data Scientist arrived on scene. I studied Signal Processing and Statistics at UNSW, and have worked in the Sydney startup scene, Cochlear and most recently at Hudson River Trading in Singapore.

I'm a Bayesian statistician, so I love impractical solutions :P, and always fail to find modern day uses. Bayesian statistics has not found a huge amount of traction in today's ML landscape, the GPU does not provide a fundamental speedup to MCMC algorithms and the additional compute power could be better directed at ensemble models or many different approaches. There are niche applications, especially in Biomathematics where the data is poor and limited and parameter spaces are large, but very limited outside academia. However, it informs my thinking and the way I approach problems.

I love programming in Python and I'm very familiar with the scientific stack. I remember hunting for pre-compiled binaries of numpy for windows in 2012, and have even contributed to numpy, [adding automatic bin estimation for histograms](https://github.com/numpy/numpy/pull/6029) plus a bunch of bugfixes since it's merge. I have dabbled in CUDA, [presenting my work on it](https://github.com/nayyarv/PyCudaIntro) with GMMs, and have hacked the CPython source code regarding binary operations for no reason whatsoever and [have a presentation on it too](https://github.com/nayyarv/CpythonLookingGlass). I do a lot of one off scripts in R, the tidyverse makes data cleaning really easy (and ggplot is the best thing ever), but I find writing larger programs is a horrid experience. I'm a better programmer than most, but I'm not a proper software engineer.

(Yes the above doubles as a "Hire Me" spiel)